## "NLP" - Natural Language Processing

### 1. Stoplist construction:

`D_stoplist.py > D_stoplist_001.txt`

The script loads dataframes including text units, thematic sections and section IDs of the _Digest_. In the preprocessing stage, the script creates a bag-of-words (`bow`) for each of the 432 thematic sections with word tokens extracted from all text units in a particular section. It removes all punctuation, leaves only one white space between words, turns the text to lower case only, and splits the string of words on white space. The list of word tokens is inserted in the `bow` column.

It imports the necessary packages, models and modules from the Classical Language Toolkit (cltk), a Python-based NLP framework for classical languages inspired by the Natural Language Toolkit (nltk).[<sup id="inline1">1</sup>](#fn1) The script initializes cltk's [`BackoffLatinLemmatizer`](http://docs.cltk.org/en/latest/latin.html#lemmatization-backoff-method) which combines multiple lemmatizer tools in a backoff chain, that is, if one tool fails to return a lemma for a word token, the token is passed on to the next tool in the chain until a lemma is returned or the chain runs out of options. The backoff method has been developed by Patrick J. Burns (University of Texas at Austin) and described in a presentation[<sup id="inline2">2</sup>](#fn2) and a review article with code snippets available as a pre-publication draft on GitHub.[<sup id="inline3">3</sup>](#fn3)

Based on the list of word tokens stored in the `bow` column, the script creates a `lemmas` column which include lists of tuples where the first element of the tuple is the token and the second is its corresponding lemma generated by cltk's `BackoffLatinLemmatizer`. The script creates a flat string of lemmas by dropping the word token and converting the list into a string. These so-called "documents" of the 432 thematic sections are used for feature extraction in the following steps. The "documents" are stored in `lem_doc` which is also inserted as a column in the dataframe.

The script imports and initializes [cltk's Latin `Stop` module](https://github.com/cltk/cltk/blob/master/cltk/stop/stop.py) developed by Patrick Burns. Burns discusses the module in the context of general challenges of stoplist construction in a research article published in 2018.[<sup id="inline4">4</sup>](#fn4) The module's `build_stoplist` method is highly customizable which takes parameters such as `texts`, `size`,  `remove_punctuation`, `remove_numbers` and `basis`. The latter parameter defines how stopwords are measured of which the simple term `frequency` meaure is used.[<sup id="inline5">5</sup>](#fn5) The initial list of most frequent terms is mnually inspected to make sure that lemmas with significant semantic value are not included in the stoplist. A list of words to retain is stored in `stop_from_stoplist` which is passed into the `build_stoplist` function as a parameter when the function is ran for the seond time to generate a _Digest_-specific `D_stoplist`.

The constructed stoplist is imported as `D_stoplist_001.txt`.

### 2. NLP pre-processing:

#### 2.1. Tokenize/lemmatize/vectorize

`NLP_sections_001.py > D_lemmatized.csv, tfidf_sections.csv, tfidf_titles.csv`

The script loads loads the necessary packages including pandas, regex, nmupy and cltk's `BackoffLatinLemmatizer`. It initilizes the lemmatizer with cltk's Latin model.

The `TfidfVectorizer` function is imported from sckit-learn. The function calculates "Term frequency-inverse document frequency" (Tfidf) scores for terms in a document (`doc`) where the document forms part of a collection of documents (`corpus`). The score indicates the term's importance in a `doc` relative to the term's importance in the `corpus`. The Tfidf score is calculated as the dot product of term t's _Term frequency_ (Tf) and its logarithmically scaled _Inverse document frequency_ (Idf) where (1) Tf is the number of times term t appears in a document divided by the total number of terms in the document and where (2) Idf is the natural logarithm of the total number of documents divided by the number of documents with term t in it.

The script loads the dataframes including text units, thematic sections and section IDs of the _Digest_ as well as the custom `D_stoplist` created with `D_stoplist.py`. After the initial merges, the text and title of each 432 thematic section is pre-processed, tokenized and lemmatized. The output is a flat string of lemmas which are stored in the new `title` and `doc` columns. During the process, Greek (non-ASCII) characters, multiple and trailing white spaces are removed and the text is converted to lower case. cltk's `BackoffLatinLemmatizer` is ran on the word tokens extracted from the titles and text units arranged in the 432 thematic sections. Stopwords stored in `D_stoplist` are removed during the process. The dataframe is streamlined with only `Section_id` (as index), `title` ("documents" of section titles) and `doc` ("documents" of thematic sections) retained. The daraframe is exported as `D_lemmatized.csv`.

The title and text of thematic sections are passed to `TfidfVectorizer` as two collections of "documents" (`corpus`) from the `title` and `doc` columns of the dataframe. The script returns two matrices: (1) one in the shape of 432 x 649 where 432 is the number of thematic sections ("documents") and 649 is the number of terms ("features") in the `corpus` of `title`, and (2) another in the shape of 432 x 10875 where 432 is the number of thematic sections ("documents") and 10875 is the number of terms ("features") in the `corpus` of `doc`. By extracting scores in an array and feature names in a list, the script builds two dataframes which include the Tfidf scores of the lemmas in the titles and texts of all 432 thematic sections. The dataframes with the Tfidf matrix are exported as `tfidf_sections.csv` and `tfidf_titles.csv`.

#### 2.2. Normalize

`NLP_sections_002.py > D_lemmatized_norm.csv, tfidf_sections_norm_top50.csv, tfidf_titles_norm.csv`

The script losds the dataframes created in the previous step and normalizes them by removing outliers and reducing dimensions.

The thematic sections are sorted by the number of unique lemmas. The average number of unique lemmas is 355.71, the median is 280.50. The percentile test shows that approximately 21% of thematic sections have less than 100 unique lemmas. These thematic sections are too short and they are likely to distort clustering and other NLP analysis. They are removed from the normalized dataframes.

An additional step is performed to reduce dimensions in the Tfidf matrix of thematic sections. In each section, 50 lemmas with the highest Tfidf score are selected and loaded to a list. After removing duplicates, the list is used to reduce the dimensions from the original 10875 lemmas to 3868.

The normalized dataframes are exported as `D_lemmatized_norm.csv`, `tfidf_sections_norm_top50.csv` and `tfidf_titles_norm.csv`.

### 3. Hierarchical clustering:

#### 3.1 Linkage matrix and dendrogram

`hierarchlust_norm_top50_001.py > norm_top50_ward_euc_clusters.npy, norm_top50_ward_euc_clusters.png`

The script loads the normalized dataframes created in the previous step. It extracts the Tfidf matrix with a shape of 340 x 3868 where 340 is the number of thematic sections which are longer than 100 unique lemmas and 3868 is the number of lemmas featuring in the thematic sections.

The script runs the `linkage_for_clustering` function defined in `pyDigest.py` which returns a dataframe with method-metric pairs for hierarchical clustering with their corresponding cophenetic correlation coefficient (CCC). The function is described in [pyDigest_documentation](https://github.com/mribary/pyDigest/blob/master/pyDigest_documentation.md#3-linkage_for_clusteringx-threshold05). The CCC-score suggests that the 'average' method combined with 'minkowski' metric produces hierarchical clustering where cluster distances are closest to the distances of individual units. With sparse data and extremely high dimensionality, method-metric pairs with high CCC-scores produce suboptimal dendrograms, that is, the tree-like plot of hierarchical clustering. Clusters are created at relatively high distances resulting in a high number of small clusters which quickly collapsed into one in the final step.

For this reason, hierarchical clustering is performed based on Ward's method with Euclidean distance. This method-metric pair produces larger clusters at lower distances which is more appropriate for Tfidf clustering with sparse data and high dimensionality. The dendrogram displayed below includes the thematic sections referenced with their IDs on the y axis and the Euclidean distance between clusters on the x axis. The plot suggests 9 larger clusters.

![Dendrogram of hierarchical clustering of tfidf_sections_norm_top50 based on Ward's method](https://github.com/mribary/pyDigest/blob/master/images/norm_top50_ward_euc_clusters.png)

The Tfidf matrix and the linkage matrix based on Ward's method is exported in a numpy binary file `norm_top50_ward_euc_clusters.npy`.

#### 3.2 Extract clusters

`hierarchlust_norm_top50_002.py > hierarchlust_norm_top50.csv`

The script loads the normalized dataframe which includes 340 thematic sections with at least 100 unique lemmas. It attaches titles to these 340 sections by loading and linkking information form another daraframe. The tfidf and linkage matrices of the 340 tematic sections are loaded from a numpy binary file.

The linkage matrix is cut by the `fcluster` method of sklearn's `cluster.hieraarchy` module. The `threshold` value is the Eucidean `distance` between clusters which are determined by inspecting the dendrogram. The table below summarises the number of clusters created at the specific `threshold` values. The 9 clusters marked with different colours in the dendrogram above are present when Euclidean distance between clusters stands at 2.5.

| Threshold (Euclidean distance) | Number of clusters at threshold |
| :--- | :--- |
| 3.5 | 2 |
| 3.0 | 4 |
| **2.5** | **9** | 
| 2.0 | 18 |
| 1.75| 29 |
| 1.625 | 43 |
| 1.50 | 55 |
| 1.375 | 75 |
| 1.3125 | 104 |
| 1.25 | 123 |

The scripts gets the cluster assignment of the 340 thematic sections at the above threshold values. It sorts the thematic sections according to their assignments at the highest to the lowest `threshold` value. The dataframe then assigns the title to each thematic sections. The tree-like hierarchical structure of clsutering is expressed by cluster assignments in the returned dataframe `hierarchlust_norm_top50.csv`.

#### 3.3 Get keywords for sections and clusters

`hierarchlust_norm_top50_003.py > hierarchlust_keywords_norm_top50.csv`

description of script

### 4. K-means

#### 4.1 K-means silhouette

`K-means_silhouette_norm_top50.py > silhouette_scores_norm_top50.txt, norm_top50_silhouette_2to75.png`

The script loads the normalized dataframe (340 theamtic sections, top 50 lemmas only) and the Tfidf matrices of sections and titles. In order to determine the number of clusters (K), the script gets the silhouette scores for clustering between a range of 2 and 75. The silhouette score measures the inner density of clusters against the outer distance between them, so that a score close 1 means perfect and 0 means totally unreliable clustering. The score 1 can only be achieved when the number of clusters (K) is equal to the number of samples being clustered.

Silhouette scores take a long time to compute, beacuse the K-means algorithm approximates its result in multiple iterations which are here set to 300. As the algorithm starts from a random state and iterations are stopped at 300, running the algorithm multiple times procduces different results. After the fifth running, the silhouette score suggests that the optimal number of clusters is 54 at a score of 0.0666. The graph below shows how the silhouette score changes as we cluster datapoints in the range between 2 and 75.

![Silhouette graph](https://github.com/mribary/pyDigest/blob/master/images/norm_top50_silhouette_2to75.png)

It must be noted that silhouette scores stay at an abmornally low level. A score which never increases above 0.1 suggest that clustering with K-means produces a very unreliable result irrespective of the number of clusters generated. K-means clustering with a mean-based algorithm at its heart is notoriously sensitive to outliers. It also performs badly with sparse high-dimensional data. This may be the reason why K-means clustering fails to produce any decent clustering.

In order to address this problem, one could experiment we two things:

(1) radically reduce the dimensionality by PCA, TruncatedSVD, t-SNE[<sup id="inline6">6</sup>](#fn6) or UMAP.

(2) abandon K-means and its spherical clustering and use an alternative method such us fuzzy K-means, K-medienas, K-medoids or an optical clustering method such as DBSCAN or HDBSCAN.

#### 4.2 K-means tested against hierarchical clustering

`K-means_norm_top50_002.py > xxx`

description

5. Notes for future reference

    which returns the ids of ten thematic sections which are most similar to the one passed for the function based on cosine similarity calculated from Tfidf scores. The script imports `linear_kernel` to calculate cosine similarity in a more economical way. 

    The following code returns the first twenty terms woth the highest Tfidf scores in the first thematic section (with id "0") which indicates the keywords of the section, that is, the terms that set the section apart from all other sections.

    ```python
    dict(df_fs.loc[0].transpose().sort_values(ascending=False).head(20))
    ```

    The stoplist, the dataframe including "documents" of lemmas (without the stopwords) extracted from the text of thematic sections, and the Tfidf matrix, of the thematic sections are exported as `D_stoplist_001.txt`, `D_tfidf_sections_001.csv` and `D_doc_sections_001.csv`.

    The script also defines a function `similar` which returns the ids of ten thematic sections which are most similar to the one passed for the function based on cosine similarity calculated from Tfidf scores. The script imports `linear_kernel` to calculate cosine similarity in a more economical way. 

### Footnotes

[<sup id="fn1">1</sup>](#inline1) Patrick J. Burns, "Building a text analysis pipeline for classical languages," in _Digital classical philology: Ancient Greek and Latin in the digital revolution_, edited by Monica Berti. Berlin: Walter de Gruyter, 2019, 159-176.

[<sup id="fn2">2</sup>](#inline2) Patrick J. Burns, "[Multiplex lemmatization with the Classical Language Toolkit](https://lila-erc.eu/wp-content/uploads/2019/06/burns-lemmatisation.pdf)," presented at the _First LiLa Workshop: Linguistic Resources & NLP Tools for Latin_ on 3 June 2019.

[<sup id="fn3">3</sup>](#inline3) Patrick J. Burns, "[Latin lemmatization: Tools, resources & future directions](https://github.com/diyclassics/lemmatizer-review/blob/master/lemmatizer-review.ipynb)," pre-publication draft available on GitHub, last updated on 3 June 2019.

[<sup id="fn4">4</sup>](#inline4) Patrick J. Burns, "[Constructing stoplists for histroical languages](https://journals.ub.uni-heidelberg.de/index.php/dco/article/view/52124/48812)," _Digital Classics Online_ 4:2 (2018): 4-20.

[<sup id="fn5">5</sup>](#inline5) The default value is `zou` which stands for the composite measure proposed by Feng Zou and his colleagues. Their measure is calculated from mean probability, variance  probability and entropy which are some of the other possible measure to be passed for `basis`. See Feng Zou, Fu Lee Wang, Xiaotie Deng, Song Han, and Lu Sheng Wang, "[Automatic Construction of Chinese Stop Word List](https://pdfs.semanticscholar.org/c543/8e216071f6180c228cc557fb1d3c77edb3a3.pdf),” In _Proceedings of the 5th WSEAS International Conference on Applied Computer Science_, 1010–1015.

[<sup id="fn6">6</sup>](#inline6) While the truncated stochastic neighbour embedding (t-SNE) method produces a visually appealing presentation of complex high-dimensional data. See the demonstration of t-SNE's power on the [distill website](https://distill.pub/2016/misread-tsne/). The main purpose of t-SNE is to create accessible presentation, but it should not be used for reducing dimensionality for the purpose of subsequent clustering. See Erich Schubert and Michael Gertz, "Intrinsic t-Stochastic Neighbor Embedding for Visualization and Outlier Detection," In _Similarity Search and Applications_. SISAP 2017. Lecture Notes in Computer Science. Vol. 10609. Edited by C. Beecks, F. Borutta, P. Kröger, T. Seidl. Berlin: Springer, 2017, 188-203.

### Notes

**Word2Vec beta**

as a keyword expander: "Keyword expansion is the taking of a query term, finding synonyms, and searching for those, too."

[link](http://docs.cltk.org/en/latest/latin.html#word2vec)

Extracted stopwords are checked against Aurelien Berra's extensive list of Latin stopwords with 4,001 items.[<sup id="inline6">6</sup>](#fn6) - [<sup id="fn6">6</sup>](#inline6) Aurélien Berra, "[Ancient Greek and Latin stopwords for textual analysis](https://github.com/aurelberra/stopwords)," Version according to the last commit on 11 November 2019 (git hash: cdf917c) published on GitHub.