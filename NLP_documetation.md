## "NLP" - Natural Language Processing

### 1. Stoplist construction:

1. D_stoplist.py > D_stoplist_001.txt

The script loads dataframes including text units, thematic sections and section IDs of the _Digest_. In the preprocessing stage, the script creates a bag-of-words (`bow`) for each of the 432 thematic sections with word tokens extracted from all text units in a particular section. It removes all punctuation, leaves only one white space between words, turns the text to lower case only, and splits the string of words on white space. The list of word tokens is inserted in the `bow` column.

It imports the necessary packages, models and modules from the Classical Language Toolkit (cltk), a Python-based NLP framework for classical languages inspired by the Natural Language Toolkit (nltk).[<sup id="inline1">1</sup>](#fn1) The script initializes cltk's [`BackoffLatinLemmatizer`](http://docs.cltk.org/en/latest/latin.html#lemmatization-backoff-method) which combines multiple lemmatizer tools in a backoff chain, that is, if one tool fails to return a lemma for a word token, the token is passed on to the next tool in the chain until a lemma is returned or the chain runs out of options. The backoff method has been developed by Patrick J. Burns (University of Texas at Austin) and described in a presentation[<sup id="inline2">2</sup>](#fn2) and a review article with code snippets available as a pre-publication draft on GitHub.[<sup id="inline3">3</sup>](#fn3)

Based on the list of word tokens stored in the `bow` column, the script creates a `lemmas` column which include lists of tuples where the first element of the tuple is the token and the second is its corresponding lemma generated by cltk's `BackoffLatinLemmatizer`. The script creates a flat string of lemmas by dropping the word token and converting the list into a string. These so-called "documents" of the 432 thematic sections are used for feature extraction in the following steps. The "documents" are stored in `lem_doc` which is also inserted as a column in the dataframe.

The script imports and initializes [cltk's Latin `Stop` module](https://github.com/cltk/cltk/blob/master/cltk/stop/stop.py) developed by Patrick Burns. Burns discusses the module in the context of general challenges of stoplist construction in a research article published in 2018.[<sup id="inline4">4</sup>](#fn4) The module's `build_stoplist` method is highly customizable which takes parameters such as `texts`, `size`,  `remove_punctuation`, `remove_numbers` and `basis`. The latter parameter defines how stopwords are measured of which the simple term `frequency` meaure is used.[<sup id="inline5">5</sup>](#fn5) The initial list of most frequent terms is mnually inspected to make sure that lemmas with significant semantic value are not included in the stoplist. A list of words to retain is stored in `stop_from_stoplist` which is passed into the `build_stoplist` function as a parameter when the function is ran for the seond time to generate a _Digest_-specific `D_stoplist`.

The constructed stoplist is imported as `D_stoplist_001.txt`.

### 2. Tokenize, lemmatize, vectorize

1. NLP_sections_001.py > D_lemmatized.csv, tfidf_sections.csv, tfidf_titles.csv

The script loads loads the necessary packages including pandas, regex, nmupy and cltk's `BackoffLatinLemmatizer`. It initilizes the lemmatizer with cltk's Latin model.

The `TfidfVectorizer` function is imported from sckit-learn. The function calculates "Term frequency-inverse document frequency" (Tfidf) scores for terms in a document (`doc`) where the document forms part of a collection of documents (`corpus`). The score indicates the term's importance in a `doc` relative to the term's importance in the `corpus`. The Tfidf score is calculated as the dot product of term t's _Term frequency_ (Tf) and its logarithmically scaled _Inverse document frequency_ (Idf) where (1) Tf is the number of times term t appears in a document divided by the total number of terms in the document and where (2) Idf is the natural logarithm of the total number of documents divided by the number of documents with term t in it.

The script loads the dataframes including text units, thematic sections and section IDs of the _Digest_ as well as the custom `D_stoplist` created with `D_stoplist.py`. After the initial merges, the text and title of each 432 thematic section is pre-processed, tokenized and lemmatized. The output is a flat string of lemmas which are stored in the new `title` and `doc` columns. During the process, Greek (non-ASCII) characters, multiple and trailing white spaces are removed and the text is converted to lower case. cltk's `BackoffLatinLemmatizer` is ran on the word tokens extracted from the titles and text units arranged in the 432 thematic sections. Stopwords stored in `D_stoplist` are removed during the process. The dataframe is streamlined with only `Section_id` (as index), `title` ("documents" of section titles) and `doc` ("documents" of thematic sections) retained. The daraframe is exported as `D_lemmatized.csv`.

The title and text of thematic sections are passed to `TfidfVectorizer` as two collections of "documents" (`corpus`) from the `title` and `doc` columns of the dataframe. The script returns two matrices: (1) one in the shape of 432 x 649 where 432 is the number of thematic sections ("documents") and 649 is the number of terms ("features") in the `corpus` of `title`, and (2) another in the shape of 432 x 10875 where 432 is the number of thematic sections ("documents") and 10875 is the number of terms ("features") in the `corpus` of `doc`. By extracting scores in an array and feature names in a list, the script builds two dataframes which include the Tfidf scores of the lemmas in the titles and texts of all 432 thematic sections. The dataframes with the Tfidf matrix are exported as `tfidf_sections.csv` and `tfidf_titles.csv`.

2. NLP_sections_002.py > 

    which returns the ids of ten thematic sections which are most similar to the one passed for the function based on cosine similarity calculated from Tfidf scores. The script imports `linear_kernel` to calculate cosine similarity in a more economical way. 

    The following code returns the first twenty terms woth the highest Tfidf scores in the first thematic section (with id "0") which indicates the keywords of the section, that is, the terms that set the section apart from all other sections.

    ```python
    dict(df_fs.loc[0].transpose().sort_values(ascending=False).head(20))
    ```

    The stoplist, the dataframe including "documents" of lemmas (without the stopwords) extracted from the text of thematic sections, and the Tfidf matrix, of the thematic sections are exported as `D_stoplist_001.txt`, `D_tfidf_sections_001.csv` and `D_doc_sections_001.csv`.

    The script also defines a function `similar` which returns the ids of ten thematic sections which are most similar to the one passed for the function based on cosine similarity calculated from Tfidf scores. The script imports `linear_kernel` to calculate cosine similarity in a more economical way. 

### Footnotes

[<sup id="fn1">1</sup>](#inline1) Patrick J. Burns, "Building a text analysis pipeline for classical languages," in _Digital classical philology: Ancient Greek and Latin in the digital revolution_, edited by Monica Berti. Berlin: Walter de Gruyter, 2019, 159-176.

[<sup id="fn2">2</sup>](#inline2) Patrick J. Burns, "[Multiplex lemmatization with the Classical Language Toolkit](https://lila-erc.eu/wp-content/uploads/2019/06/burns-lemmatisation.pdf)," presented at the _First LiLa Workshop: Linguistic Resources & NLP Tools for Latin_ on 3 June 2019.

[<sup id="fn3">3</sup>](#inline3) Patrick J. Burns, "[Latin lemmatization: Tools, resources & future directions](https://github.com/diyclassics/lemmatizer-review/blob/master/lemmatizer-review.ipynb)," pre-publication draft available on GitHub, last updated on 3 June 2019.

[<sup id="fn4">4</sup>](#inline4) Patrick J. Burns, "[Constructing stoplists for histroical languages](https://journals.ub.uni-heidelberg.de/index.php/dco/article/view/52124/48812)," _Digital Classics Online_ 4:2 (2018): 4-20.

[<sup id="fn5">5</sup>](#inline5) The default value is `zou` which stands for the composite measure proposed by Feng Zou and his colleagues. Their measure is calculated from mean probability, variance  probability and entropy which are some of the other possible measure to be passed for `basis`. See Feng Zou, Fu Lee Wang, Xiaotie Deng, Song Han, and Lu Sheng Wang, "[Automatic Construction of Chinese Stop Word List](https://pdfs.semanticscholar.org/c543/8e216071f6180c228cc557fb1d3c77edb3a3.pdf),” In _Proceedings of the 5th WSEAS International Conference on Applied Computer Science_, 1010–1015.

### Notes

**Word2Vec beta**

as a keyword expander: "Keyword expansion is the taking of a query term, finding synonyms, and searching for those, too."

[link](http://docs.cltk.org/en/latest/latin.html#word2vec)

Extracted stopwords are checked against Aurelien Berra's extensive list of Latin stopwords with 4,001 items.[<sup id="inline6">6</sup>](#fn6) - [<sup id="fn6">6</sup>](#inline6) Aurélien Berra, "[Ancient Greek and Latin stopwords for textual analysis](https://github.com/aurelberra/stopwords)," Version according to the last commit on 11 November 2019 (git hash: cdf917c) published on GitHub.