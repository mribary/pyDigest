{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37564bit13fddfa0140645c199f4c0ad8a176c2c",
   "display_name": "Python 3.7.5 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import basic packages\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataframes from GitHub repo\n",
    "file_path_df = 'https://raw.githubusercontent.com/mribary/pyDigest/master/input/Ddf_v105.csv'\n",
    "file_path_s = 'https://raw.githubusercontent.com/mribary/pyDigest/master/input/Ddf_sections_v001.csv'\n",
    "file_path_sID = 'https://raw.githubusercontent.com/mribary/pyDigest/master/input/Ddf_Section_IDs_v001.csv'\n",
    "df = pd.read_csv(file_path_df, index_col=0)\n",
    "s = pd.read_csv(file_path_s, index_col=0)\n",
    "sID = pd.read_csv(file_path_sID, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge dataframes and keep only necessary columns\n",
    "df_1 = df.TextUnit\n",
    "s_1 = s.Section_id\n",
    "s_df = pd.merge(s_1, df_1, left_index=True, right_index=True)\n",
    "s_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# Bag of words from thematic sections with preprocessing\n",
    "punctuation = r\"[\\\"#$%&\\'()*+,-/:;<=>@[\\]^_`{|}~.?!«»]\"             # Punctuation pattern\n",
    "bow_sections = []\n",
    "for i in range(len(s_df.Section_id.unique())):\n",
    "    text = str(list(s_df.TextUnit[s_df.Section_id == i]))           # Load all text units from a thematic unit\n",
    "    text_no_punct = re.sub(punctuation, '', text)                   # Remove punctuation\n",
    "    text_one_white_space = re.sub(r\"\\s{2,}\", ' ', text_no_punct)    # Leave only one white space between words\n",
    "    text_lower = text_one_white_space.lower()                       # Transform to all lower case\n",
    "    text_split = text_lower.split(' ')                              # Split to a list of tokens\n",
    "    bow_sections.append(text_split)                                 # Load word tokens into a list\n",
    "sID['bow'] = bow_sections                                           # Insert bow in a new column in sID\n",
    "sID.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import packages and models from cltk and initialize tools\n",
    "from cltk.corpus.utils.importer import CorpusImporter\n",
    "from cltk.lemmatize.latin.backoff import BackoffLatinLemmatizer\n",
    "corpus_importer = CorpusImporter('latin')                           # Initialize cltk's CorpusImporter\n",
    "corpus_importer.import_corpus('latin_models_cltk')                  # Import the latin_models_cltk corpus for lemmatization\n",
    "lemmatizer = BackoffLatinLemmatizer()                               # Initialize Latin lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column with tuples of (token, lemma)\n",
    "lemmas_list = []\n",
    "for x in sID.Section_id:\n",
    "    tokens = sID.loc[sID.Section_id == x,'bow'].values[0]           # Load tokens from the bow column\n",
    "    lemmas = lemmatizer.lemmatize(tokens)                           # Lemmatize tokens\n",
    "    lemmas_list.append(lemmas)                                      # Load lemmas into a list\n",
    "sID['lemmas'] = lemmas_list                                         # Insert lemmas in a new column\n",
    "sID.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of \"documents\" from lemmas for vectorization\n",
    "lem_doc = []\n",
    "for x in sID.Section_id:\n",
    "    lemmas = sID.loc[sID.Section_id == x,'lemmas'].values[0]        # Load tokens from the bow column\n",
    "    l = []                                                          # Create empty list for lemmas in one row\n",
    "    for y in range(len(lemmas)):\n",
    "        l.append(lemmas[y][1])                                      # Load the lemma (and drop the token) to the list\n",
    "    l_string = ' '.join([str(word) for word in l])                  # Create a string from the list: \"document\"\n",
    "    lem_doc.append(l_string)                                        # Add the \"document\" to a list\n",
    "sID['lem_doc'] = lem_doc                                            # Insert list of \"documents\" in a new column\n",
    "sID.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Latin stoplist with cltk based on frequency\n",
    "from cltk.stop.latin import CorpusStoplist  # Import the Latin stop module\n",
    "S = CorpusStoplist()                        # Initialize cltk's Latin stop module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_stoplist_initial = S.build_stoplist(lem_doc, basis='frequency', size=150, inc_values=True, sort_words=False)\n",
    "# List of words from initial stoplist to be exluded from stoplist\n",
    "stop_from_stoplist = ['accipio', 'actio', 'actium', 'ago', 'bonus', 'causa', 'condicio', 'creditor', 'debeo', 'dico', \\\n",
    "    'dies', 'dominus', 'emo', 'facio', 'familia', 'fideicommitto', 'filius', 'fio', 'fundus', 'habeo', 'hereditas', \\\n",
    "    'heres', 'iudicium', 'ius', 'legatus', 'lego', 'lex', 'liber', 'libertas', 'licet', 'locus', 'meus', 'mulier', 'multus', \\\n",
    "    'nomen', 'oportet', 'pars', 'paruus', 'pater', 'pecunia', 'pertineo', 'peto', 'possessio', 'praesto', 'praetor', 'puto', \\\n",
    "    'quaero', 'ratio', 'relinquo', 'res', 'respondeo', 'restituo', 'scribo', 'servus', 'solvo', 'stipulo', 'tempus', \\\n",
    "    'teneo', 'testamentum', 'utor', 'verus', 'video', 'volo']\n",
    "# Adjusted stoplist\n",
    "D_stoplist = S.build_stoplist(lem_doc, basis='frequency', size=120, inc_values=True, sort_words=False, exclude=stop_from_stoplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and initialize TfidfVecotirizer with custom stoplist\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(analyzer='word', stop_words=D_stoplist)   # Initialize Tfidf vecitorizer with stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize section titles based on a corpus of 432 documents including the 432 section titles\n",
    "corpus = lem_doc                                # Define corpus as set of \"documents\" from section titles\n",
    "X = vectorizer.fit_transform(corpus)            # Vectorize: dtype: matrix, shape: (432, 10925)\n",
    "scores = X.toarray().transpose()                # Create and transpose array: dtype: numpy array, shape: (10925, 432)\n",
    "feature_names = vectorizer.get_feature_names()  # Extract the feature names for the Tfidf dictionary: dtype: list, len: 10925"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary with lemmas as keys and Tfidf scores in section titles as values\n",
    "feature_scores = dict(zip(feature_names, scores))\n",
    "\n",
    "# Create dataframe for lemmas and their Tfidf scores\n",
    "df_fs = pd.DataFrame(feature_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first 20 lemmas with the highest Tfidf scores in a given thematic section\n",
    "dict(df_fs.loc[0].transpose().sort_values(ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create recommender for 10 most similar thematic sections based on cosine similarity\n",
    "from sklearn.metrics.pairwise import linear_kernel                     # Import cosine_similarity (as linear_kernel)\n",
    "corpus = lem_doc                                # Define corpus as set of \"documents\" from section titles\n",
    "X = vectorizer.fit_transform(corpus)            # Generate Tfidf matrix: X\n",
    "cosine_sim = linear_kernel(X, X)                # Generate cosine similarity matrix: cosine_sim\n",
    "def similar(id, cosine_sim):\n",
    "    # Sort thematic sections based on the similarity scores\n",
    "    sim_scores = list(enumerate(cosine_sim[id]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    # Get the scores for 10 most similar thematic sections\n",
    "    sim_scores = sim_scores[1:11]\n",
    "    return sim_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar(0, cosine_sim)                          # List of tuples (id, similarity score)"
   ]
  }
 ]
}